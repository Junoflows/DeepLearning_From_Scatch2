{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kOPGoL4PyA-TK7o6Jtjh70aSL8PBYSmI",
      "authorship_tag": "ABX9TyOjbilpIxSB1lDB7Vp9YR+v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Junoflows/Deeplearning_From_Scatch2/blob/main/Chapter_3_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 3 word2vec"
      ],
      "metadata": {
        "id": "IZoFAHFLuZtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 앞서 통계 기반 기법으로 단어의 분산 표현을 얻었는데 이번에는 더 강력한 기법인 추론 기반 기법을 살펴보자."
      ],
      "metadata": {
        "id": "qb8K8hk7ucZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 추론 기반 기법과 신경망"
      ],
      "metadata": {
        "id": "o8744pFzum9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 단어를 벡터로 표현하는 방법에는 크게 통계 기반 기법과 추론 기반 기법이 있다.\n",
        "+ 앞서 본 통계 기반 기법의 문제를 지적하고 그 대안인 추론 기반 기법의 이점을 살펴보자.\n",
        "+ word2vec의 전처리를 위해 신경망으로 단어를 처리하는 예를 살펴보자."
      ],
      "metadata": {
        "id": "146Um_Bbuo3S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.1 통계 기반 기법의 문제점"
      ],
      "metadata": {
        "id": "NoKwhoAHu502"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 통계 기반 기법에서는 주변 단어의 빈도를 기초로 단어를 표현했다.\n",
        "+ 단어의 동시발생 행렬을 만들고 SVD를 적용하여 밀집벡터를 얻었다.\n",
        "+ 이 방식은 대규모 말뭉치를 다룰 때 문제가 발생한다.\n",
        "+ 영어의 어휘 수는 100만이 넘는데, 이를 통계 기반 기법에서는 100만×100만이라는 행렬을 만들게 된다.\n",
        "+ 여기에 SVD를 적용하는 일은 현실적이지 않다.\n"
      ],
      "metadata": {
        "id": "bgp--ez8u7ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 통계 기반 기법은 말뭉치 전체의 통계를 이용해 단 1회의 처리만에 단어의 분산 표현을 얻는다.\n",
        "+ 추론 기반 기법에서 신경망을 이용하는 경우는 미니배치로 학습하는 것이 일반적이다.\n",
        "+ 아래 그림은 두 기법의 차이를 보여준다."
      ],
      "metadata": {
        "id": "S_AtDsXIvxVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1TtNewPZkM3sc80gVjmDTuMOr9F_mHL0h' width = 550/><br>"
      ],
      "metadata": {
        "id": "GRrH1VjXwQ1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위처럼 통계 기반 기법은 학습 데이터를 한꺼번에 처리한다.(배치 학습)\n",
        "+ 추론 기반 기법은 학습 데이터의 일부를 사용해 순차적으로 학습한다.(미니배치 학습)\n",
        "+ 즉 말뭉치의 어휘 수가 많아 계산량이 큰 작업을 처리하기 어려운 경우도 신경망을 학습시킬 수 있다는 뜻이다.\n",
        "+ 여러 머신과 GPU를 이용한 병렬 계산도 가능해져서 학습 속도를 높일 수도 있다."
      ],
      "metadata": {
        "id": "AYKw3_A3wTxu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.2 추론 기반 기법 개요"
      ],
      "metadata": {
        "id": "G_GRPegdwvgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 추론이 주된 작업으로, 추론이란 아래처럼 주변 단어가 주어졌을 때 ? 에 무슨 단어가 들어갈 지 추측하는 작업이다."
      ],
      "metadata": {
        "id": "cjPDKRBhz7d7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1mra4EEP5SR1BXDV16o4U-rADXRQhYOQD' width = 550/><br>"
      ],
      "metadata": {
        "id": "5481TaFc0ex_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 추론 문제를 풀고 학습하는 것이 추론 기반 기법이 다루는 문제이다.\n",
        "+ 추론 문제를 반복해서 풀면서 단어의 출현 패턴을 학습하는 것이다.\n",
        "+ 모델 관점에서 보면 추론 문제는 아래처럼 보인다."
      ],
      "metadata": {
        "id": "qenn8lhH0hRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1EkQ088S4hRO9P8Wu0w9w19YGZ0owLQ9I' width =550/><br>"
      ],
      "metadata": {
        "id": "Bo9SJ3Rj1DGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위에서 보듯 추론 기반 기법에는 모델이 등장하는데 모델로 신경망을 사용한다.\n",
        "+ 모델은 맥락 정보를 입력받아 각 단어의 출현 확률을 출력한다.\n",
        "+ 이러한 틀 안에서 말뭉치를 사용해 모델이 올바른 추측을 내놓도록 학습시킨다.\n",
        "+ 이 학습의 결과로 단어의 분산 표현을 얻는 것이 추론 기반 기법의 전체 그림이다."
      ],
      "metadata": {
        "id": "6MtJTxFM1Owa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1.3 신경망에서의 단어 처리"
      ],
      "metadata": {
        "id": "ifaVorI31qHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 신경망은 you와 say 등의 단어를 처리할 수 없으니 단어를 고정 길이 벡터로 변환해야 한다.\n",
        "+ 이때 벡터의 원소 하나만 1이고 나머지는 0인 벡터로 하는 원핫 벡터로 변환하는 방법을 사용한다.\n",
        "+ You say goodbye and I say hello. 라는 문장을 원핫 벡터에 대해 살펴보자.\n",
        "+ (\"you\", \"say\", \"goodbye\", \"and\", \"I\", \"hello\",\".\") 이중 두 단어의 원핫 벡터를 나타내면 아래와 같다."
      ],
      "metadata": {
        "id": "hYb3iJUz1r36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1NfVLVqlVVNw886QBJVjWmipzHIdhhF6D' width = 550/><br>"
      ],
      "metadata": {
        "id": "87xrtDep2jkg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 단어를 원핫 벡터로 변환하는 방법은 총 어휘 수만큼 원소를 갖는 벡터를 준비하고  \n",
        "인덱스가 단어 ID와 같은 원소를 1, 나머지를 0으로 설정한다.\n",
        "+ 이처럼 단어를 고정 길이 벡터로 변환하면 신경망의 입력층은 뉴런의 수를 고정할 수 있다."
      ],
      "metadata": {
        "id": "H8HZOqZn20yJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1TaezhpshVAi1_UkpHPPpDRb192n-m9eK' width = 550 /><br>"
      ],
      "metadata": {
        "id": "mHlTgGRD3UFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림처럼 입력층의 뉴런은 총 7개이고, 차례로 7개의 단어들에 대응한다.\n",
        "+ 즉 단어를 벡터로 나타낼 수 있고, 신경망을 구성하는 계층들은 벡터를 처리할 수 있으니 단어를 신경망으로 처리할 수 있다는 뜻이다.\n",
        "+ 아래 그림은 원핫 표현으로 된 단어 하나를 완전연결계층을 통해 변환하는 모습을 보여준다."
      ],
      "metadata": {
        "id": "S9rAh9un3aUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Jrm_gW7F7cJeD91B-z3ZHCUtYKiKQL0c' width =550/><br>"
      ],
      "metadata": {
        "id": "MyyK71-15wBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 신경망은 완전연결계층이므로 각각의 노드가 이웃 층의 모든 노드와 화살표로 연결되어 있다.\n",
        "+ 화살표에는 가중치가 존재하여, 입력층 뉴런과의 가중합이 은닉층 뉴런이 된다.\n",
        "+ 앞으로 가중치를 명확하게 보여주기 위해 다음과 같이 그린다."
      ],
      "metadata": {
        "id": "B7BGM9uc6JLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1lMrJl_Q4Yzgme7ugzRivx5I3v16WNvT0' width =550 /><br>"
      ],
      "metadata": {
        "id": "tHsv88Hd66DT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 지금까지 내용을 코드로 살펴보자.\n",
        "+ 완전연결계층의 의한 변환은 파이썬으로 다음과 같다."
      ],
      "metadata": {
        "id": "K4qe6RbR69Zn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNNg4aYDuMv8",
        "outputId": "c6bf360e-30c0-42ea-80ce-c356b24d67b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.03487317 -0.61523164 -0.10167472]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "c = np.array([1, 0, 0, 0, 0, 0, 0])   # 입력\n",
        "W = np.random.randn(7, 3)             # 가중치\n",
        "h = np.matmul(c, W)                   # 중간 노드\n",
        "print(h)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 단어 ID가 0인 단어를 원핫 표현으로 표현한 후 완전연결계층을 통과시켜 반환하였다.\n",
        "+ c는 원핫 표현이므로 단어 ID에 대응하는 원소만 1이고 그 외에는 0인 벡터이다.\n",
        "+ c와 W의 행렬 곱은 아래처럼 가중치의 행벡터 하나를 뽑아낸 것과 같다."
      ],
      "metadata": {
        "id": "2gt3odo_7zi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1_8D1GKYjZf4gNYMDh96L5W8Nmq5pUu4V' width = 550/><br>"
      ],
      "metadata": {
        "id": "KbzghTt-8M0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 앞의 코드로 수행한 작업은 1장에서 구현한 MatMul 계층으로도 수행할 수 있다."
      ],
      "metadata": {
        "id": "HFGdEZLW8cbX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/deep-learning-from-scratch-2-master')\n",
        "import numpy as np\n",
        "from common.layers import MatMul\n",
        "\n",
        "c = np.array([1, 0, 0, 0, 0, 0, 0])\n",
        "W = np.random.randn(7, 3)\n",
        "layer = MatMul(W)\n",
        "h = layer.forward(c)\n",
        "print(h)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkdjU0mU7WHP",
        "outputId": "39163e52-7cef-494f-ee8b-f93fceb2624e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.50006898  0.11183846 -0.14706295]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ MatMul 계층에 가중치 W를 설정하고 forward()를 호출해 순전파를 수행한다."
      ],
      "metadata": {
        "id": "EotNMLfO9HOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 단순한 word2vec"
      ],
      "metadata": {
        "id": "qBRdAHLg9W4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ word2vec을 구현해보자.\n",
        "+ 여기서 사용할 신경망은 word2vec에서 제안하는 CBOW 모델이다."
      ],
      "metadata": {
        "id": "gePRXNj19Y6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.1 CBOW 모델의 추론 처리"
      ],
      "metadata": {
        "id": "kcPCj2ey9i6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델은 맥락으로부터 타겟을 추측하는 용도의 신경망이다.(타겟:중앙단어, 맥락:주변단어)\n",
        "+ CBOW 모델이 가능한 한 정확하게 추론하도록 훈련시켜서 단어의 분산 표현을 얻어낸다.\n",
        "+ CBOW 모델의 입력은 맥락으로 you, goodbye 같은 단어들의 목록이다.\n",
        "+ 이 맥락을 원핫 벡터로 변환하여 CBOW 모델이 처리할 수 있도록 한다."
      ],
      "metadata": {
        "id": "dfDWgfV9_oBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1yJTYo-sTdpBto2HamqpmJw43gt8hxBg8' width = 550/><br>"
      ],
      "metadata": {
        "id": "tCS0OBqfAJAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림은 CBOW 모델의 신경망으로 입력층이 2개 있고, 은닉층을 거쳐 출력층에 도달한다.\n",
        "+ 두 입력층에서 은닉층으로의 변환은 같은 완전연결계층이 처리한다.\n",
        "+ 은닉층에서 출력층 뉴런으로의 변환은 다른 완전연결계층이 처리한다."
      ],
      "metadata": {
        "id": "tQRkfZ1-AUzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__NOTE__ <br/>\n",
        "입력층이 2개인 이유는 맥락으로 고려할 단어를 2개로 정했기 떄문이다."
      ],
      "metadata": {
        "id": "K0GQhoKMAvNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 은닉층의 뉴런은 입력층의 완전연결계층에 의해 변환된 값이 되는데, 입력층이 여러 개이면 전체를 평균하면 된다.\n",
        "+ 앞의 예에 대입해보면 다음과 같다.\n",
        "+ 완전연결계층에 의한 첫 번째 입력층이 $h_1$으로 변환되고 두 번째 입력층이 $h_2$로 변환되었으면  \n",
        "은닉층 뉴런은 $\\frac{1}{2}(h_1+h_2)$가 된다."
      ],
      "metadata": {
        "id": "WVCC6cGfA0nH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 출력층을 살펴보면 뉴런은 총 7개인데, 이 뉴런 하나하나가 각각의 단어에 대응한다.\n",
        "+ 츨력층의 뉴런은 각 단어의 점수를 뜻하며, 값이 높을수록 대응 단어의 출현 확률도 높아진다.\n",
        "+ 여기서 점수를 소프트맥스 함수를 적용하여 확률을 얻을 수 있다."
      ],
      "metadata": {
        "id": "LWGbV0FT9GNF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 입력층에서 은닉층으로의 변환은 완전연결계층에 의해 이뤄지고 이때 가중치 $W_{in}$은 7×3 행렬이다.\n",
        "+ 이 가중치가 단어의 분산 표현이다."
      ],
      "metadata": {
        "id": "-DOoD8KP9UqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1s4iQQvaSPk4UIU1kLh22Izbt2pzEcgCp' width= 550 /><br>"
      ],
      "metadata": {
        "id": "sJ9mHYqX9j0F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 가중치 $W_{in}$의 각 행에는 해당 단어의 분산 표현이 담겨있어서  \n",
        "학습이 진행될수록 맥락의 단어를 잘 추측하는 방향으로 분산 표현들이 갱신된다.\n",
        "+ 이렇게 해서 얻은 벡터는 단어의 의미도 잘 나타낸다."
      ],
      "metadata": {
        "id": "RgP1dKeg92z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 지금까지 CBOW 모델을 뉴런 관점에서 살펴보았는데 이번에는 계층 관점에서 살펴보자."
      ],
      "metadata": {
        "id": "x5VHqSGk-LEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1b7fJc0hos4TjEi4v7zbNn1f7NKiMxDcD' width = 550/><br>"
      ],
      "metadata": {
        "id": "p4fcQQRf-OtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델의 가장 앞에는 2개의 MatMul 계층이 있고 이 두 계층의 출력이 더해진다.\n",
        "+ 더해진 값의 0.5를 곱하여 생긴 평균이 은닉층의 뉴런이 된다.\n",
        "+ 은닉층 뉴런에 MatMul 계층이 적용되어 점수가 출력된다."
      ],
      "metadata": {
        "id": "02DOKP6W-lQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 그림을 참고하여 CBOW 모델의 추론 처리를 파이썬으로 구현해보자."
      ],
      "metadata": {
        "id": "op29PKJj-1Xy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/deep-learning-from-scratch-2-master')\n",
        "import numpy as np\n",
        "from common.layers import MatMul\n",
        "\n",
        "\n",
        "# 샘플 맥락 데이터\n",
        "c0 = np.array([[1, 0, 0, 0, 0, 0, 0]])\n",
        "c1 = np.array([[0, 0, 1, 0, 0, 0, 0]])\n",
        "\n",
        "# 가중치 초기화\n",
        "W_in = np.random.randn(7, 3)\n",
        "W_out = np.random.randn(3, 7)\n",
        "\n",
        "# 계층 생성\n",
        "in_layer0 = MatMul(W_in)\n",
        "in_layer1 = MatMul(W_in)\n",
        "out_layer = MatMul(W_out)\n",
        "\n",
        "# 순전파\n",
        "h0 = in_layer0.forward(c0)\n",
        "h1 = in_layer1.forward(c1)\n",
        "h = 0.5 * (h0 + h1)\n",
        "s = out_layer.forward(h)\n",
        "\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXZb8c7t-lx8",
        "outputId": "227a1d1a-f66d-4f72-b1d7-1d4047cf2eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.57879104  0.40983182 -0.81408926  0.43445798 -0.45221447  0.13664619\n",
            "   0.24453247]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 먼저 필요한 가중치들을 초기화한 후, 입력층을 처리하는 MatMul 계층을 맥락 수 만큼 생성한다.\n",
        "+ 출력층에는 MatMul 계층을 1개만 생성하고, 입력층의 MatMul 게층은 $W_{in}$ 가중치를 공유한다.\n",
        "+ 입력층의 MatMul 계층들의 forward()를 호출해 중간 데이터를 계산하고 출력층 MatMul 계층을 통과시켜 각 단어의 점수를 구한다.\n",
        "+ 이상이 CBOW 모델의 추론 과정이며 계속해 CBOW 모델의 학습에 대해 알아보자."
      ],
      "metadata": {
        "id": "hf07D7hJBNUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.2 CBOW 모델의 학습"
      ],
      "metadata": {
        "id": "O7F7Uss1B7Aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델의 출력층에서 각 단어의 점수를 출력하였고 소프트맥스 함수를 적용하면 확률을 얻을 수 있다.\n",
        "+ 이 확률은 맥락이 주어졌을 때 중앙에 어떤 단어가 나오는지를 나타낸다."
      ],
      "metadata": {
        "id": "QHZ8dwhMB9IK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 아래의 예에서 맥락은 you, goodbye 이고 정답레이블은 say이다.\n",
        "+ 가중치가 잘 설정된 신경망에서 뉴런의 값이 가장 큰 것이 정답을 나타내는 뉴런이라고 예상할 수 있다."
      ],
      "metadata": {
        "id": "HdEFivGVCME8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1yfSSnA0y0TbEDlID2BRx5h0883JDjNw6' width = 550/><br>"
      ],
      "metadata": {
        "id": "y2X4QUjGCjBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델의 학습에서는 올바른 예측을 할 수 있도록 가중치를 조정한다.  \n",
        "+ 가중치 $W_{in}$에 단어의 출현 패턴을 파악한 벡터가 학습된다."
      ],
      "metadata": {
        "id": "hEwrlMPLCoRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델로 얻을 수 있는 단어의 분산 표현은 의미와 문법 측면에서 우리의 직관에 부합한다."
      ],
      "metadata": {
        "id": "nbrHATO8C72F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 다중 클래스 분류를 수행하는 신경망을 학습하려면 소프트맥스와 교차 엔트로피 오차를 이용한다.\n",
        "+ 소프트맥스 함수를로 점수를 확률로 변환하고, 이 확률과 정답 레이블의 교차 엔트로피 오차를 구한 후  \n",
        "그 값을 손실로 사용해 학습을 진행한다."
      ],
      "metadata": {
        "id": "F8YXG0xnDNQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1YkxKVqBxy8OONzI7FGqSgRQB-cp5yDa3' width = 550/><br>"
      ],
      "metadata": {
        "id": "bB31lW4FEiSp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 앞서 추론 처리 CBOW 모델에 Softmax 계층과 Cross Entropy Error 계층만 추가했다.\n",
        "+ 이것만으로 손실을 얻을 수 있고, 이상이 CBOW 모델의 손실을 구하는 과정이며 순방향 전파이다.\n",
        "+ Softmax 계층과 Cross Entropy Error 계층을 Softmax with Loss 라는 하나의 계층으로 구현하자."
      ],
      "metadata": {
        "id": "nLJ205frEj7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1nKueyb3e-K8CRwtOERXLnYfJi1hrBCEb' width = 550/><br>"
      ],
      "metadata": {
        "id": "gnd9x3BaFGH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2.3 word2vec의 가중치와 분산 표현"
      ],
      "metadata": {
        "id": "lzrqy_bnFHyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ word2vec에 사용되는 신경망은 입력층 가중치 $W_{in}$과 출력층 가중치 $W_{out}$ 두 가지가 있다.\n",
        "+ $W_{in}$의 행이 각 단어의 분산 표현에 해당하고 $W_{out}$에도 단어의 의미가 인코딩된 벡터가 저장되어 있다.\n",
        "+ 출력 측 가중치는 각 단어의 분산 표현이 열 방향으로 저장된다."
      ],
      "metadata": {
        "id": "a9GKqbH4FK0X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1nSFsRE5B0kdlH34E6DlooSMEOG7w_Gcj' width = 550/><br>"
      ],
      "metadata": {
        "id": "_aRkql1fHFHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 최종적으로 이용하는 단어의 분산 표현으로는 입력 측의 가중치만 이용하는게 가장 대중적인 선택이다.\n",
        "+ 따라서 $W_{in}$을 단어의 분산 표현으로 이용한다."
      ],
      "metadata": {
        "id": "RydHryM9HPA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 학습 데이터 준비"
      ],
      "metadata": {
        "id": "Ohce8YZYHv22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ word2vec 학습에 쓰일 학습 데이터를 준비하기 앞서 간단한 예로  \n",
        "You say goodbye and I say hello 라는 한 문장 말뭉치를 이용해보자."
      ],
      "metadata": {
        "id": "KjhXUjFMHyWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.1 맥락과 타깃"
      ],
      "metadata": {
        "id": "eWxjyLaxH_hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ word2vec에서 신경망의 입력은 맥락이고 정답 레이블은 맥락에 싸인 중앙의 단어, 타겟이다.\n",
        "+ 우리는 신경망에 맥락을 입력했을 때 타깃이 출현할 확률을 높이도록 학습해야 한다."
      ],
      "metadata": {
        "id": "QBqGonceIBRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1T5F79adBaOanE0THTegXLWDVqOREGFzW' width = 550/><br>"
      ],
      "metadata": {
        "id": "s-_WagNyIQMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 말뭉치에서 목표단어를 타깃으로 주변 단어를 맥락으로 뽑았다.\n",
        "+ 이 작업을 말뭉치 안의 모든 단어에 대해 수행한 결과가 오른쪽의 맥락과 타깃이다.\n",
        "+ 맥락의 각 행이 신경망의 입력으로, 타깃의 각 행이 정답 레이블이 된다.\n",
        "+ 각 샘플 데이터에서 맥락의 수는 window_size로 설정할 수 있고(예에서는 2개) 타깃은 하나이다.\n"
      ],
      "metadata": {
        "id": "YqFYr2WrIYpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 말뭉치로부터 맥락과 타깃을 만드는 함수를 구현해보자.\n",
        "+ 말뭉치 텍스트를 단어 ID로 변환해야 하는데 앞서 2장에서 구현한 preprocess 함수를 사용한다."
      ],
      "metadata": {
        "id": "KaXU7UnWIzZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/deep-learning-from-scratch-2-master')\n",
        "from common.util import preprocess\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "print(corpus)\n",
        "print(id_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezlIbxDtA8Ad",
        "outputId": "083318a8-87af-485b-f975-12b4f99ef38f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 2 3 4 1 5 6]\n",
            "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 단어의 ID 배열인 corpus로부터 맥락과 타깃을 만들어낸다."
      ],
      "metadata": {
        "id": "GNUjUL7qJWDt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=16mwvuh7e00L6IHVQeu-YgjO_UKn6j2J5' width= 550/><br>"
      ],
      "metadata": {
        "id": "ZPRlCHjKJbHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 맥락은 2차원 배열이고 맥락의 0번째 차원에는 각 맥락 데이터가 저장된다.\n",
        "+ 즉 contexts[i]에는 i번째 맥락이 저장되고 마찬가지로 타깃에서도 target[i]에 i번째 타깃이 저장된다."
      ],
      "metadata": {
        "id": "dr9v3KohJj5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 맥락과 타깃을 만드는 함수를 구현해보자."
      ],
      "metadata": {
        "id": "oHl-oG_9J13H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_contexts_target(corpus, window_size=1):\n",
        "    target = corpus[window_size:-window_size]\n",
        "    contexts = []\n",
        "\n",
        "    for idx in range(window_size, len(corpus)-window_size):\n",
        "        cs = []\n",
        "        for t in range(-window_size, window_size + 1):\n",
        "            if t == 0:\n",
        "                continue\n",
        "            cs.append(corpus[idx + t])\n",
        "        contexts.append(cs)\n",
        "\n",
        "    return np.array(contexts), np.array(target)"
      ],
      "metadata": {
        "id": "kA6FsgoNJSeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이 함수는 단어 ID의 배열(corpus), 맥락의 윈도우 크기(window_size) 두 개의 인수를 받는다.\n",
        "+ 맥락과 타깃을 각각 넘파이 다차원 배열로 저장한다.\n",
        "+ 이 함수를 사용해보면 다음과 같다."
      ],
      "metadata": {
        "id": "CA6QAemMKIJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "contexts, target = create_contexts_target(corpus, window_size = 1)\n",
        "print(contexts)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6vtsS5QKHoM",
        "outputId": "b2b097fd-0944-44af-9b56-096675bf86d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 2]\n",
            " [1 3]\n",
            " [2 4]\n",
            " [3 1]\n",
            " [4 5]\n",
            " [1 6]]\n",
            "[1 2 3 4 1 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이것으로 말뭉치엣 맥락과 타깃을 만들었고 이를 CBOW 모델에 넘겨주면 된다.\n",
        "+ 맥락과 타깃의 각 원소가 단어 ID이므로 이를 원핫 표현으로 변환하자."
      ],
      "metadata": {
        "id": "-jOi2H2jKo4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3.2 원핫 표현으로 변환"
      ],
      "metadata": {
        "id": "oBUbCKouK0dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 맥락과 타깃을 원핫 표현으로 바꾸는 과정은 다음과 같다."
      ],
      "metadata": {
        "id": "p7GN8SfGK2Kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1O9jrjI75qbopULC7zPadyK3xxBnxW0OG' width = 550/><br>"
      ],
      "metadata": {
        "id": "ITuGEDl0LQr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 맥락과 타깃을 단어 ID에서 원핫 표현으로 변환하는데 각각 다차원 배열의 형상을 살펴보자.\n",
        "+ 단어 ID에서의 맥락의 형상은 (6,2)인데 원핫으로 표현하면 (6,2,7)이 된다.\n"
      ],
      "metadata": {
        "id": "lhiY4pMFLSBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 원핫 표현으로 변환은 convert_one_hot() 함수를 사용하며 인수로는 단어 ID 목록과 어휘 수를 받는다.\n",
        "+ 지금까지의 데이터 준비 과정을 모아서 정리해보자."
      ],
      "metadata": {
        "id": "h4ZwvtYHLkB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/deep-learning-from-scratch-2-master')\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "contexts, target = create_contexts_target(corpus, window_size = 1)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)"
      ],
      "metadata": {
        "id": "_nKaxRpAKlSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 CBOW 모델 구현"
      ],
      "metadata": {
        "id": "nvpVRKMuMFI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 앞서 보았듯이 우리가 구현할 신경망은 다음과 같다."
      ],
      "metadata": {
        "id": "FGpvQDBEMHz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=18LSGfXlO1jmd3nqAkrpbn26FCxcZM0fn' width = 550/><br>"
      ],
      "metadata": {
        "id": "tjsvJlxnMXCs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 신경망을 SimpleCBOW 라는 이름으로 구현한다.\n",
        "+ 초기화 메서드부터 시작해보자."
      ],
      "metadata": {
        "id": "vOkCfbX4MY9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/deep-learning-from-scratch-2-master')\n",
        "import numpy as np\n",
        "from common.layers import MatMul, SoftmaxWithLoss\n",
        "\n",
        "\n",
        "class SimpleCBOW:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        V, H = vocab_size, hidden_size\n",
        "\n",
        "        # 가중치 초기화\n",
        "        W_in = 0.01 * np.random.randn(V, H).astype('f')\n",
        "        W_out = 0.01 * np.random.randn(H, V).astype('f')\n",
        "\n",
        "        # 계층 생성\n",
        "        self.in_layer0 = MatMul(W_in)\n",
        "        self.in_layer1 = MatMul(W_in)\n",
        "        self.out_layer = MatMul(W_out)\n",
        "        self.loss_layer = SoftmaxWithLoss()\n",
        "\n",
        "        # 모든 가중치와 기울기를 리스트에 모은다.\n",
        "        layers = [self.in_layer0, self.in_layer1, self.out_layer]\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "        # 인스턴스 변수에 단어의 분산 표현을 저장한다.\n",
        "        self.word_vecs = W_in"
      ],
      "metadata": {
        "id": "w3pnBO0gMDn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 초기화 메서드는 인수로 어휘 수와 은닉층의 뉴런 수를 받는다.\n",
        "+ 가중치 초기화에서는 가중치 2개를 작은 무작위 값으로 초기화하여 32비트 부동소수점 수로 생성한다.\n",
        "+ 입력층의 MatMul 계층을 2개, 출력층의 MatMul 계층을 하나, Softmax with Loss 계층을 하나 생성한다.\n",
        "+ 입력층의 맥락을 처리하는 MatMul 계층은 맥락에서 사용하는 단어의 수(윈도우 크기)만큼 만들어야 한다.\n",
        "+ 입력층의 MatMul 계층들은 같은 가중치를 이용하도록 초기화한다.\n",
        "+ 마지막으로 신경망에서 사용되는 매개변수와 기울기를 params와 grads 리스트에 각각 모은다."
      ],
      "metadata": {
        "id": "Bl993t3cMyXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이어서 순전파인 forward()를 구현한다.\n",
        "+ 인수로 맥락(contexts)과 타깃(target)을 받아서 손실(loss)을 반환한다."
      ],
      "metadata": {
        "id": "d9f7pkNdNo1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def forward(self, contexts, target):\n",
        "        h0 = self.in_layer0.forward(contexts[:, 0])\n",
        "        h1 = self.in_layer1.forward(contexts[:, 1])\n",
        "        h = (h0 + h1) * 0.5\n",
        "        score = self.out_layer.forward(h)\n",
        "        loss = self.loss_layer.forward(score, target)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "si3u7CI6NBk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 역전파인 backward()를 구현하자.\n",
        "+ 역전파의 계산 그래프는 다음과 같다."
      ],
      "metadata": {
        "id": "aUFQZlZ3N_jV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1oK2XKlbonYHZsVCwnbh1NLEjv2U8N8nq' width =550 /><br>"
      ],
      "metadata": {
        "id": "p8D2DW76N5RZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 역전파는 1에서 시작하여 바로 Softmax with Loss 계층에 입력된다.\n",
        "+ Softmax with Loss 계층의 역전파 출력이 ds이며 이를 출력층 MatMul 계층으로 입력한다.\n",
        "+ 그 후 ×와 + 연산으로 역전파된다.\n",
        "+ ×는 순전파 시 입력을 서로 바꿔 기울기에 곱하고 +는 기울기를 그대로 통과시킨다."
      ],
      "metadata": {
        "id": "IIseJJgmONdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def backward(self, dout=1):\n",
        "        ds = self.loss_layer.backward(dout)\n",
        "        da = self.out_layer.backward(ds)\n",
        "        da *= 0.5\n",
        "        self.in_layer1.backward(da)\n",
        "        self.in_layer0.backward(da)\n",
        "        return None"
      ],
      "metadata": {
        "id": "fkvx66ULN31T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이것으로 역전파 구현까지 마쳤다.\n",
        "+ 각 매개변수의 기울기를 grads에 모아뒀으므로 forward() 호출한 후 backward()를 호출하는 것 만으로 grads 리스트의 기울기가 갱신된다.\n",
        "+ SimpleCBOW 클래스의 학습을 살펴보자."
      ],
      "metadata": {
        "id": "TvKuKIG7OsP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4.1 학습 코드 구현"
      ],
      "metadata": {
        "id": "eB19Fy3eO_MM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델의 학습은 일반적인 신경망의 학습과 완전히 같다.\n",
        "+ 학습 데이터를 준비해 신경망에 입력한 다음 기울기를 구하고가중치 매개변수를 순서대로 갱신해간다.\n",
        "+ 이번 학습 과정을 수행하는데 1장에서의 Trainer 클래스를 사용한다."
      ],
      "metadata": {
        "id": "TPU-foh0PCFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 매개변수 갱신 방법으로는 Adam을 사용했다."
      ],
      "metadata": {
        "id": "0WezUUZPPkXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/deep-learning-from-scratch-2-master')\n",
        "from common.trainer import Trainer\n",
        "from common.optimizer import Adam\n",
        "from ch03.simple_cbow import SimpleCBOW\n",
        "from common.util import preprocess, create_contexts_target, convert_one_hot\n",
        "\n",
        "\n",
        "window_size = 1\n",
        "hidden_size = 5\n",
        "batch_size = 3\n",
        "max_epoch = 1000\n",
        "\n",
        "text = 'You say goodbye and I say hello.'\n",
        "corpus, word_to_id, id_to_word = preprocess(text)\n",
        "\n",
        "vocab_size = len(word_to_id)\n",
        "contexts, target = create_contexts_target(corpus, window_size)\n",
        "target = convert_one_hot(target, vocab_size)\n",
        "contexts = convert_one_hot(contexts, vocab_size)\n",
        "\n",
        "model = SimpleCBOW(vocab_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "\n",
        "trainer.fit(contexts, target, max_epoch, batch_size)\n",
        "trainer.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WMIeBSxrOrxk",
        "outputId": "5c3c1b3f-4baf-445b-83d3-e82b180eac99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| 에폭 1 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 2 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 3 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 4 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 5 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 6 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 7 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 8 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 9 |  반복 1 / 2 | 시간 0[s] | 손실 1.95\n",
            "| 에폭 10 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 11 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 12 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 13 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 14 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 15 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 16 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 17 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 18 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 19 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 20 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 21 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 22 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 23 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 24 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 25 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 26 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 27 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 28 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 29 |  반복 1 / 2 | 시간 0[s] | 손실 1.94\n",
            "| 에폭 30 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 31 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 32 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 33 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 34 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 35 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 36 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 37 |  반복 1 / 2 | 시간 0[s] | 손실 1.93\n",
            "| 에폭 38 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 39 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 40 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 41 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 42 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 43 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 44 |  반복 1 / 2 | 시간 0[s] | 손실 1.92\n",
            "| 에폭 45 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 46 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 47 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 48 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 49 |  반복 1 / 2 | 시간 0[s] | 손실 1.91\n",
            "| 에폭 50 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 51 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 52 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 53 |  반복 1 / 2 | 시간 0[s] | 손실 1.89\n",
            "| 에폭 54 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 55 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 56 |  반복 1 / 2 | 시간 0[s] | 손실 1.90\n",
            "| 에폭 57 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 58 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 59 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 60 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 61 |  반복 1 / 2 | 시간 0[s] | 손실 1.88\n",
            "| 에폭 62 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 63 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 64 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
            "| 에폭 65 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 66 |  반복 1 / 2 | 시간 0[s] | 손실 1.87\n",
            "| 에폭 67 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
            "| 에폭 68 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 69 |  반복 1 / 2 | 시간 0[s] | 손실 1.86\n",
            "| 에폭 70 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 71 |  반복 1 / 2 | 시간 0[s] | 손실 1.85\n",
            "| 에폭 72 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 73 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 74 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 75 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 76 |  반복 1 / 2 | 시간 0[s] | 손실 1.84\n",
            "| 에폭 77 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 78 |  반복 1 / 2 | 시간 0[s] | 손실 1.81\n",
            "| 에폭 79 |  반복 1 / 2 | 시간 0[s] | 손실 1.83\n",
            "| 에폭 80 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 81 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 82 |  반복 1 / 2 | 시간 0[s] | 손실 1.82\n",
            "| 에폭 83 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 84 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 85 |  반복 1 / 2 | 시간 0[s] | 손실 1.79\n",
            "| 에폭 86 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 87 |  반복 1 / 2 | 시간 0[s] | 손실 1.80\n",
            "| 에폭 88 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
            "| 에폭 89 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 90 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 91 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 92 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
            "| 에폭 93 |  반복 1 / 2 | 시간 0[s] | 손실 1.76\n",
            "| 에폭 94 |  반복 1 / 2 | 시간 0[s] | 손실 1.77\n",
            "| 에폭 95 |  반복 1 / 2 | 시간 0[s] | 손실 1.75\n",
            "| 에폭 96 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 97 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 98 |  반복 1 / 2 | 시간 0[s] | 손실 1.78\n",
            "| 에폭 99 |  반복 1 / 2 | 시간 0[s] | 손실 1.70\n",
            "| 에폭 100 |  반복 1 / 2 | 시간 0[s] | 손실 1.72\n",
            "| 에폭 101 |  반복 1 / 2 | 시간 0[s] | 손실 1.74\n",
            "| 에폭 102 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 103 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 104 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 105 |  반복 1 / 2 | 시간 0[s] | 손실 1.71\n",
            "| 에폭 106 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 107 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 108 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 109 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
            "| 에폭 110 |  반복 1 / 2 | 시간 0[s] | 손실 1.67\n",
            "| 에폭 111 |  반복 1 / 2 | 시간 0[s] | 손실 1.68\n",
            "| 에폭 112 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 113 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
            "| 에폭 114 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
            "| 에폭 115 |  반복 1 / 2 | 시간 0[s] | 손실 1.69\n",
            "| 에폭 116 |  반복 1 / 2 | 시간 0[s] | 손실 1.66\n",
            "| 에폭 117 |  반복 1 / 2 | 시간 0[s] | 손실 1.61\n",
            "| 에폭 118 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
            "| 에폭 119 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 120 |  반복 1 / 2 | 시간 0[s] | 손실 1.60\n",
            "| 에폭 121 |  반복 1 / 2 | 시간 0[s] | 손실 1.65\n",
            "| 에폭 122 |  반복 1 / 2 | 시간 0[s] | 손실 1.63\n",
            "| 에폭 123 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
            "| 에폭 124 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 125 |  반복 1 / 2 | 시간 0[s] | 손실 1.62\n",
            "| 에폭 126 |  반복 1 / 2 | 시간 0[s] | 손실 1.64\n",
            "| 에폭 127 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 128 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 129 |  반복 1 / 2 | 시간 0[s] | 손실 1.57\n",
            "| 에폭 130 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
            "| 에폭 131 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 132 |  반복 1 / 2 | 시간 0[s] | 손실 1.58\n",
            "| 에폭 133 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
            "| 에폭 134 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
            "| 에폭 135 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
            "| 에폭 136 |  반복 1 / 2 | 시간 0[s] | 손실 1.59\n",
            "| 에폭 137 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 138 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
            "| 에폭 139 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 140 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 141 |  반복 1 / 2 | 시간 0[s] | 손실 1.52\n",
            "| 에폭 142 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 143 |  반복 1 / 2 | 시간 0[s] | 손실 1.56\n",
            "| 에폭 144 |  반복 1 / 2 | 시간 0[s] | 손실 1.50\n",
            "| 에폭 145 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
            "| 에폭 146 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 147 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 148 |  반복 1 / 2 | 시간 0[s] | 손실 1.49\n",
            "| 에폭 149 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 150 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 151 |  반복 1 / 2 | 시간 0[s] | 손실 1.55\n",
            "| 에폭 152 |  반복 1 / 2 | 시간 0[s] | 손실 1.43\n",
            "| 에폭 153 |  반복 1 / 2 | 시간 0[s] | 손실 1.51\n",
            "| 에폭 154 |  반복 1 / 2 | 시간 0[s] | 손실 1.47\n",
            "| 에폭 155 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
            "| 에폭 156 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 157 |  반복 1 / 2 | 시간 0[s] | 손실 1.46\n",
            "| 에폭 158 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 159 |  반복 1 / 2 | 시간 0[s] | 손실 1.48\n",
            "| 에폭 160 |  반복 1 / 2 | 시간 0[s] | 손실 1.42\n",
            "| 에폭 161 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 162 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 163 |  반복 1 / 2 | 시간 0[s] | 손실 1.53\n",
            "| 에폭 164 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 165 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 166 |  반복 1 / 2 | 시간 0[s] | 손실 1.45\n",
            "| 에폭 167 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 168 |  반복 1 / 2 | 시간 0[s] | 손실 1.38\n",
            "| 에폭 169 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 170 |  반복 1 / 2 | 시간 0[s] | 손실 1.40\n",
            "| 에폭 171 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 172 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 173 |  반복 1 / 2 | 시간 0[s] | 손실 1.36\n",
            "| 에폭 174 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 175 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 176 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 177 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 178 |  반복 1 / 2 | 시간 0[s] | 손실 1.37\n",
            "| 에폭 179 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 180 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 181 |  반복 1 / 2 | 시간 0[s] | 손실 1.34\n",
            "| 에폭 182 |  반복 1 / 2 | 시간 0[s] | 손실 1.39\n",
            "| 에폭 183 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 184 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 185 |  반복 1 / 2 | 시간 0[s] | 손실 1.26\n",
            "| 에폭 186 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 187 |  반복 1 / 2 | 시간 0[s] | 손실 1.41\n",
            "| 에폭 188 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 189 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 190 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 191 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 192 |  반복 1 / 2 | 시간 0[s] | 손실 1.32\n",
            "| 에폭 193 |  반복 1 / 2 | 시간 0[s] | 손실 1.27\n",
            "| 에폭 194 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
            "| 에폭 195 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 196 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 197 |  반복 1 / 2 | 시간 0[s] | 손실 1.30\n",
            "| 에폭 198 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 199 |  반복 1 / 2 | 시간 0[s] | 손실 1.31\n",
            "| 에폭 200 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 201 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 202 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 203 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 204 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 205 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 206 |  반복 1 / 2 | 시간 0[s] | 손실 1.29\n",
            "| 에폭 207 |  반복 1 / 2 | 시간 0[s] | 손실 1.28\n",
            "| 에폭 208 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 209 |  반복 1 / 2 | 시간 0[s] | 손실 1.35\n",
            "| 에폭 210 |  반복 1 / 2 | 시간 0[s] | 손실 1.14\n",
            "| 에폭 211 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 212 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 213 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
            "| 에폭 214 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 215 |  반복 1 / 2 | 시간 0[s] | 손실 1.19\n",
            "| 에폭 216 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 217 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 218 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 219 |  반복 1 / 2 | 시간 0[s] | 손실 1.16\n",
            "| 에폭 220 |  반복 1 / 2 | 시간 0[s] | 손실 1.17\n",
            "| 에폭 221 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 222 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 223 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 224 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 225 |  반복 1 / 2 | 시간 0[s] | 손실 1.22\n",
            "| 에폭 226 |  반복 1 / 2 | 시간 0[s] | 손실 1.21\n",
            "| 에폭 227 |  반복 1 / 2 | 시간 0[s] | 손실 1.15\n",
            "| 에폭 228 |  반복 1 / 2 | 시간 0[s] | 손실 1.07\n",
            "| 에폭 229 |  반복 1 / 2 | 시간 0[s] | 손실 1.20\n",
            "| 에폭 230 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 231 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 232 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 233 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 234 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 235 |  반복 1 / 2 | 시간 0[s] | 손실 1.18\n",
            "| 에폭 236 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 237 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 238 |  반복 1 / 2 | 시간 0[s] | 손실 1.10\n",
            "| 에폭 239 |  반복 1 / 2 | 시간 0[s] | 손실 1.24\n",
            "| 에폭 240 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 241 |  반복 1 / 2 | 시간 0[s] | 손실 1.25\n",
            "| 에폭 242 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 243 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 244 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 245 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 246 |  반복 1 / 2 | 시간 0[s] | 손실 1.23\n",
            "| 에폭 247 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 248 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 249 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 250 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 251 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 252 |  반복 1 / 2 | 시간 0[s] | 손실 1.13\n",
            "| 에폭 253 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 254 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 255 |  반복 1 / 2 | 시간 0[s] | 손실 1.12\n",
            "| 에폭 256 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 257 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 258 |  반복 1 / 2 | 시간 0[s] | 손실 1.06\n",
            "| 에폭 259 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 260 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 261 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 262 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 263 |  반복 1 / 2 | 시간 0[s] | 손실 1.09\n",
            "| 에폭 264 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 265 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 266 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 267 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 268 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 269 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 270 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 271 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 272 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 273 |  반복 1 / 2 | 시간 0[s] | 손실 1.04\n",
            "| 에폭 274 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 275 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 276 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 277 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 278 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 279 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 280 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 281 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 282 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 283 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 284 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 285 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 286 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 287 |  반복 1 / 2 | 시간 0[s] | 손실 1.11\n",
            "| 에폭 288 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 289 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 290 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 291 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 292 |  반복 1 / 2 | 시간 0[s] | 손실 1.08\n",
            "| 에폭 293 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 294 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 295 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 296 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 297 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 298 |  반복 1 / 2 | 시간 0[s] | 손실 1.01\n",
            "| 에폭 299 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 300 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 301 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 302 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 303 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 304 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 305 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 306 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 307 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 308 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 309 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 310 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 311 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 312 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 313 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 314 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 315 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 316 |  반복 1 / 2 | 시간 0[s] | 손실 1.05\n",
            "| 에폭 317 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 318 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 319 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 320 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 321 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 322 |  반복 1 / 2 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 323 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 324 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 325 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 326 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 327 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 328 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 329 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 330 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 331 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 332 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 333 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 334 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 335 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 336 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 337 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 338 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 339 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 340 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 341 |  반복 1 / 2 | 시간 0[s] | 손실 0.96\n",
            "| 에폭 342 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 343 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 344 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 345 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 346 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 347 |  반복 1 / 2 | 시간 0[s] | 손실 1.03\n",
            "| 에폭 348 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 349 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 350 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 351 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 352 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 353 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 354 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 355 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 356 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 357 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 358 |  반복 1 / 2 | 시간 0[s] | 손실 1.02\n",
            "| 에폭 359 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 360 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 361 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 362 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 363 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 364 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 365 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 366 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 367 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 368 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 369 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 370 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 371 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 372 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 373 |  반복 1 / 2 | 시간 0[s] | 손실 0.94\n",
            "| 에폭 374 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 375 |  반복 1 / 2 | 시간 0[s] | 손실 0.93\n",
            "| 에폭 376 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 377 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 378 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 379 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 380 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 381 |  반복 1 / 2 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 382 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 383 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 384 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 385 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 386 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 387 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 388 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 389 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 390 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 391 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 392 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 393 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 394 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 395 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 396 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 397 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 398 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 399 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 400 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 401 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 402 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 403 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 404 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 405 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 406 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 407 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 408 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 409 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 410 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 411 |  반복 1 / 2 | 시간 0[s] | 손실 0.92\n",
            "| 에폭 412 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 413 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 414 |  반복 1 / 2 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 415 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 416 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 417 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 418 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 419 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 420 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 421 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 422 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 423 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 424 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 425 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 426 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 427 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 428 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 429 |  반복 1 / 2 | 시간 0[s] | 손실 0.88\n",
            "| 에폭 430 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 431 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 432 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 433 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 434 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 435 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 436 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 437 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 438 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 439 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 440 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 441 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 442 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 443 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 444 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 445 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 446 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 447 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 448 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 449 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 450 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 451 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 452 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 453 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 454 |  반복 1 / 2 | 시간 0[s] | 손실 0.95\n",
            "| 에폭 455 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 456 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 457 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 458 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 459 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 460 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 461 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 462 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 463 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 464 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 465 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 466 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 467 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 468 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 469 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 470 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 471 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 472 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 473 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 474 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 475 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 476 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 477 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 478 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 479 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 480 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 481 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 482 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 483 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 484 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 485 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 486 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 487 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 488 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 489 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 490 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 491 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 492 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 493 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 494 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 495 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 496 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 497 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 498 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 499 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 500 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 501 |  반복 1 / 2 | 시간 0[s] | 손실 0.89\n",
            "| 에폭 502 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 503 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 504 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 505 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 506 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 507 |  반복 1 / 2 | 시간 0[s] | 손실 0.99\n",
            "| 에폭 508 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 509 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 510 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 511 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 512 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 513 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 514 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 515 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 516 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 517 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 518 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 519 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 520 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 521 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 522 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 523 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 524 |  반복 1 / 2 | 시간 0[s] | 손실 0.91\n",
            "| 에폭 525 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 526 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 527 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 528 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 529 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 530 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 531 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 532 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 533 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 534 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 535 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 536 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 537 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 538 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 539 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 540 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 541 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 542 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 543 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 544 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 545 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 546 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 547 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 548 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 549 |  반복 1 / 2 | 시간 0[s] | 손실 0.90\n",
            "| 에폭 550 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 551 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 552 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 553 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 554 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 555 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 556 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 557 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 558 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 559 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 560 |  반복 1 / 2 | 시간 0[s] | 손실 0.83\n",
            "| 에폭 561 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 562 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 563 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 564 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 565 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 566 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 567 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 568 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 569 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 570 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 571 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 572 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 573 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 574 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 575 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 576 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 577 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 578 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 579 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 580 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 581 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 582 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 583 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 584 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 585 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 586 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 587 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 588 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 589 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 590 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 591 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 592 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 593 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 594 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 595 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 596 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 597 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 598 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 599 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 600 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 601 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 602 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 603 |  반복 1 / 2 | 시간 0[s] | 손실 0.82\n",
            "| 에폭 604 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 605 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 606 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 607 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 608 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 609 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 610 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 611 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 612 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 613 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 614 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 615 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 616 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 617 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 618 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 619 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 620 |  반복 1 / 2 | 시간 0[s] | 손실 0.84\n",
            "| 에폭 621 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 622 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 623 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 624 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 625 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 626 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 627 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 628 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 629 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 630 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 631 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 632 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 633 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 634 |  반복 1 / 2 | 시간 0[s] | 손실 0.81\n",
            "| 에폭 635 |  반복 1 / 2 | 시간 0[s] | 손실 0.39\n",
            "| 에폭 636 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 637 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 638 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 639 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 640 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 641 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 642 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 643 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 644 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 645 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 646 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 647 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 648 |  반복 1 / 2 | 시간 0[s] | 손실 0.76\n",
            "| 에폭 649 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 650 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 651 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 652 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 653 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 654 |  반복 1 / 2 | 시간 0[s] | 손실 0.87\n",
            "| 에폭 655 |  반복 1 / 2 | 시간 0[s] | 손실 0.38\n",
            "| 에폭 656 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 657 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 658 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 659 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 660 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 661 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 662 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 663 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 664 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 665 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 666 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 667 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 668 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 669 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 670 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 671 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 672 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 673 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 674 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 675 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 676 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 677 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 678 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 679 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 680 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 681 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 682 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 683 |  반복 1 / 2 | 시간 0[s] | 손실 0.44\n",
            "| 에폭 684 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 685 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 686 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 687 |  반복 1 / 2 | 시간 0[s] | 손실 0.86\n",
            "| 에폭 688 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
            "| 에폭 689 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 690 |  반복 1 / 2 | 시간 0[s] | 손실 0.62\n",
            "| 에폭 691 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 692 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 693 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 694 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 695 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
            "| 에폭 696 |  반복 1 / 2 | 시간 0[s] | 손실 0.75\n",
            "| 에폭 697 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 698 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 699 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 700 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 701 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 702 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 703 |  반복 1 / 2 | 시간 0[s] | 손실 0.74\n",
            "| 에폭 704 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 705 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 706 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 707 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 708 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 709 |  반복 1 / 2 | 시간 0[s] | 손실 0.43\n",
            "| 에폭 710 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 711 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 712 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 713 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 714 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 715 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 716 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 717 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 718 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 719 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 720 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 721 |  반복 1 / 2 | 시간 0[s] | 손실 0.69\n",
            "| 에폭 722 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 723 |  반복 1 / 2 | 시간 0[s] | 손실 0.61\n",
            "| 에폭 724 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 725 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
            "| 에폭 726 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 727 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 728 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 729 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 730 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 731 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 732 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 733 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 734 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 735 |  반복 1 / 2 | 시간 0[s] | 손실 0.49\n",
            "| 에폭 736 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 737 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 738 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 739 |  반복 1 / 2 | 시간 0[s] | 손실 0.85\n",
            "| 에폭 740 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
            "| 에폭 741 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 742 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 743 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 744 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 745 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 746 |  반복 1 / 2 | 시간 0[s] | 손실 0.80\n",
            "| 에폭 747 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
            "| 에폭 748 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 749 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 750 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 751 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 752 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 753 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 754 |  반복 1 / 2 | 시간 0[s] | 손실 0.42\n",
            "| 에폭 755 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 756 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 757 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 758 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 759 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 760 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 761 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 762 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 763 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 764 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 765 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 766 |  반복 1 / 2 | 시간 0[s] | 손실 0.55\n",
            "| 에폭 767 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 768 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 769 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 770 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 771 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 772 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 773 |  반복 1 / 2 | 시간 0[s] | 손실 0.41\n",
            "| 에폭 774 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 775 |  반복 1 / 2 | 시간 0[s] | 손실 0.68\n",
            "| 에폭 776 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 777 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 778 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 779 |  반복 1 / 2 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 780 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
            "| 에폭 781 |  반복 1 / 2 | 시간 0[s] | 손실 0.66\n",
            "| 에폭 782 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 783 |  반복 1 / 2 | 시간 0[s] | 손실 0.79\n",
            "| 에폭 784 |  반복 1 / 2 | 시간 0[s] | 손실 0.48\n",
            "| 에폭 785 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 786 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 787 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 788 |  반복 1 / 2 | 시간 0[s] | 손실 0.67\n",
            "| 에폭 789 |  반복 1 / 2 | 시간 0[s] | 손실 0.53\n",
            "| 에폭 790 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 791 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 792 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 793 |  반복 1 / 2 | 시간 0[s] | 손실 0.57\n",
            "| 에폭 794 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 795 |  반복 1 / 2 | 시간 0[s] | 손실 0.47\n",
            "| 에폭 796 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 797 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 798 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 799 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 800 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 801 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 802 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 803 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 804 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 805 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 806 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
            "| 에폭 807 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 808 |  반복 1 / 2 | 시간 0[s] | 손실 0.64\n",
            "| 에폭 809 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 810 |  반복 1 / 2 | 시간 0[s] | 손실 0.59\n",
            "| 에폭 811 |  반복 1 / 2 | 시간 0[s] | 손실 0.50\n",
            "| 에폭 812 |  반복 1 / 2 | 시간 0[s] | 손실 0.72\n",
            "| 에폭 813 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 814 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 815 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 816 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 817 |  반복 1 / 2 | 시간 0[s] | 손실 0.77\n",
            "| 에폭 818 |  반복 1 / 2 | 시간 0[s] | 손실 0.45\n",
            "| 에폭 819 |  반복 1 / 2 | 시간 0[s] | 손실 0.78\n",
            "| 에폭 820 |  반복 1 / 2 | 시간 0[s] | 손실 0.34\n",
            "| 에폭 821 |  반복 1 / 2 | 시간 0[s] | 손실 0.63\n",
            "| 에폭 822 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 823 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 824 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 825 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 826 |  반복 1 / 2 | 시간 0[s] | 손실 0.54\n",
            "| 에폭 827 |  반복 1 / 2 | 시간 0[s] | 손실 0.70\n",
            "| 에폭 828 |  반복 1 / 2 | 시간 0[s] | 손실 0.52\n",
            "| 에폭 829 |  반복 1 / 2 | 시간 0[s] | 손실 0.46\n",
            "| 에폭 830 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 831 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 832 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 833 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 834 |  반복 1 / 2 | 시간 0[s] | 손실 0.60\n",
            "| 에폭 835 |  반복 1 / 2 | 시간 0[s] | 손실 0.56\n",
            "| 에폭 836 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 837 |  반복 1 / 2 | 시간 0[s] | 손실 0.58\n",
            "| 에폭 838 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 839 |  반복 1 / 2 | 시간 0[s] | 손실 0.51\n",
            "| 에폭 840 |  반복 1 / 2 | 시간 0[s] | 손실 0.65\n",
            "| 에폭 841 |  반복 1 / 2 | 시간 0[s] | 손실 0.40\n",
            "| 에폭 842 |  반복 1 / 2 | 시간 0[s] | 손실 0.71\n",
            "| 에폭 843 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 844 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 845 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 846 |  반복 1 / 2 | 시간 1[s] | 손실 0.40\n",
            "| 에폭 847 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 848 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 849 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 850 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 851 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
            "| 에폭 852 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 853 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 854 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 855 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 856 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 857 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 858 |  반복 1 / 2 | 시간 1[s] | 손실 0.46\n",
            "| 에폭 859 |  반복 1 / 2 | 시간 1[s] | 손실 0.76\n",
            "| 에폭 860 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 861 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 862 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 863 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 864 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 865 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 866 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 867 |  반복 1 / 2 | 시간 1[s] | 손실 0.71\n",
            "| 에폭 868 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 869 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 870 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 871 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 872 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 873 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 874 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 875 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 876 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 877 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 878 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
            "| 에폭 879 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 880 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 881 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 882 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 883 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 884 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 885 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 886 |  반복 1 / 2 | 시간 1[s] | 손실 0.45\n",
            "| 에폭 887 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 888 |  반복 1 / 2 | 시간 1[s] | 손실 0.59\n",
            "| 에폭 889 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 890 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 891 |  반복 1 / 2 | 시간 1[s] | 손실 0.39\n",
            "| 에폭 892 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 893 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 894 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 895 |  반복 1 / 2 | 시간 1[s] | 손실 0.52\n",
            "| 에폭 896 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 897 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 898 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 899 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 900 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 901 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 902 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 903 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 904 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 905 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 906 |  반복 1 / 2 | 시간 1[s] | 손실 0.70\n",
            "| 에폭 907 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
            "| 에폭 908 |  반복 1 / 2 | 시간 1[s] | 손실 0.75\n",
            "| 에폭 909 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 910 |  반복 1 / 2 | 시간 1[s] | 손실 0.32\n",
            "| 에폭 911 |  반복 1 / 2 | 시간 1[s] | 손실 0.74\n",
            "| 에폭 912 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 913 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 914 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 915 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 916 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 917 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 918 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 919 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 920 |  반복 1 / 2 | 시간 1[s] | 손실 0.51\n",
            "| 에폭 921 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 922 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 923 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 924 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 925 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 926 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 927 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 928 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 929 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 930 |  반복 1 / 2 | 시간 1[s] | 손실 0.31\n",
            "| 에폭 931 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 932 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 933 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 934 |  반복 1 / 2 | 시간 1[s] | 손실 0.56\n",
            "| 에폭 935 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 936 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 937 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 938 |  반복 1 / 2 | 시간 1[s] | 손실 0.63\n",
            "| 에폭 939 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 940 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 941 |  반복 1 / 2 | 시간 1[s] | 손실 0.43\n",
            "| 에폭 942 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 943 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 944 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 945 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
            "| 에폭 946 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 947 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 948 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 949 |  반복 1 / 2 | 시간 1[s] | 손실 0.67\n",
            "| 에폭 950 |  반복 1 / 2 | 시간 1[s] | 손실 0.38\n",
            "| 에폭 951 |  반복 1 / 2 | 시간 1[s] | 손실 0.69\n",
            "| 에폭 952 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 953 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 954 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 955 |  반복 1 / 2 | 시간 1[s] | 손실 0.64\n",
            "| 에폭 956 |  반복 1 / 2 | 시간 1[s] | 손실 0.47\n",
            "| 에폭 957 |  반복 1 / 2 | 시간 1[s] | 손실 0.57\n",
            "| 에폭 958 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 959 |  반복 1 / 2 | 시간 1[s] | 손실 0.60\n",
            "| 에폭 960 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 961 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 962 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 963 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 964 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 965 |  반복 1 / 2 | 시간 1[s] | 손실 0.80\n",
            "| 에폭 966 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 967 |  반복 1 / 2 | 시간 1[s] | 손실 0.54\n",
            "| 에폭 968 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 969 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 970 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 971 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 972 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 973 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 974 |  반복 1 / 2 | 시간 1[s] | 손실 0.62\n",
            "| 에폭 975 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 976 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 977 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 978 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 979 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 980 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 981 |  반복 1 / 2 | 시간 1[s] | 손실 0.44\n",
            "| 에폭 982 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 983 |  반복 1 / 2 | 시간 1[s] | 손실 0.79\n",
            "| 에폭 984 |  반복 1 / 2 | 시간 1[s] | 손실 0.48\n",
            "| 에폭 985 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 986 |  반복 1 / 2 | 시간 1[s] | 손실 0.72\n",
            "| 에폭 987 |  반복 1 / 2 | 시간 1[s] | 손실 0.37\n",
            "| 에폭 988 |  반복 1 / 2 | 시간 1[s] | 손실 0.55\n",
            "| 에폭 989 |  반복 1 / 2 | 시간 1[s] | 손실 0.68\n",
            "| 에폭 990 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 991 |  반복 1 / 2 | 시간 1[s] | 손실 0.58\n",
            "| 에폭 992 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 993 |  반복 1 / 2 | 시간 1[s] | 손실 0.61\n",
            "| 에폭 994 |  반복 1 / 2 | 시간 1[s] | 손실 0.42\n",
            "| 에폭 995 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 996 |  반복 1 / 2 | 시간 1[s] | 손실 0.49\n",
            "| 에폭 997 |  반복 1 / 2 | 시간 1[s] | 손실 0.53\n",
            "| 에폭 998 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n",
            "| 에폭 999 |  반복 1 / 2 | 시간 1[s] | 손실 0.66\n",
            "| 에폭 1000 |  반복 1 / 2 | 시간 1[s] | 손실 0.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48152 (\\N{HANGUL SYLLABLE BAN}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48373 (\\N{HANGUL SYLLABLE BOG}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49552 (\\N{HANGUL SYLLABLE SON}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49892 (\\N{HANGUL SYLLABLE SIL}) missing from current font.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 학습을 거듭할수록 손실이 줄어드는 것을 알 수 있다.\n",
        "+ 학습이 끝난 후 가중치 매개변수를 살펴보자.\n",
        "+ 입력층 MatMul 계층의 가중치를 꺼내 실제 내용을 확인해보자."
      ],
      "metadata": {
        "id": "NUKChGKxPutk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vecs = model.word_vecs\n",
        "for word_id, word in id_to_word.items():\n",
        "    print(word, word_vecs[word_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3OvEejSPYqO",
        "outputId": "29ffac4c-c0fc-4929-a9d8-851263ac7adf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you [ 1.0135729  1.0294467 -1.0343095  1.2663658  1.3385414]\n",
            "say [-1.2501268  -1.247404    1.2725587  -0.10856082  0.04438563]\n",
            "goodbye [ 0.98090607  1.0015186  -0.90608174  0.58291394  0.48628062]\n",
            "and [-1.0862287 -1.1125846  1.1349623 -1.5972226 -1.6234853]\n",
            "i [ 0.9672954   0.9858275  -0.91421175  0.5812488   0.48802358]\n",
            "hello [ 1.044033   1.0417709 -1.0133272  1.2602041  1.3545943]\n",
            ". [-1.0225208 -0.9248858  1.040392   1.4062419  1.4054738]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ word_vecs로 가중치를 꺼내는데 각 행에는 대응하는 단어 ID의 분산 표현이 저장돼 있다."
      ],
      "metadata": {
        "id": "v47VYqFhP_TU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이로써 단어를 밀집벡터로 나타냈는데 이 밀집벡터가 단어의 분산 표현이다.\n",
        "+ 하지만 작은 말뭉치로는 좋은 결과를 얻을 수 없다.\n",
        "+ 큰 말뭉치로 바꾸면 결과는 좋아지지만 처리 속도에서 문제가 생기게 되는데 4장에서 CBOW 모델을 개선하여 구현할 예정이다."
      ],
      "metadata": {
        "id": "CJaZ0YcOQE7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 word2vec 보충"
      ],
      "metadata": {
        "id": "hkzMZlNHQaBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이번에는 word2vec에 관한 중요한 주제 몇 가지를 살펴보자.\n",
        "+ 우선 CBOW 모델을 확률 관점에서 다시 살펴보자."
      ],
      "metadata": {
        "id": "0wrUA2vcSceY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.1 CBOW 모델과 확률"
      ],
      "metadata": {
        "id": "byDZNNp8SkyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 우선 확률 표기법을 간단하게 살펴보자.\n",
        "+ A라는 현상이 일어날 확률은 P(A)로 쓰고, 동시 확률은 P(A, B)로 쓴다.\n",
        "+ 사후 확률은 B가 주어졌을 때 A가 일어날 확률로 P(A | B)로 쓴다."
      ],
      "metadata": {
        "id": "y1IhfAqmSmzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델을 확률 표기법으로 기술해보자.\n",
        "+ CBOW 모델이 하는 일은 맥락을 주면 타깃 단어가 출현할 확률을 출력하는 것이다.\n",
        "+ 말뭉치를 $w_1, w_2, ... , w_r$처럼 단어 시퀀스로 표기하고 t번쨰 단어에 대해 윈도우 크기가 1인 맥락을 고려한다."
      ],
      "metadata": {
        "id": "uWa2kDAaS_AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1vQuM1ZmE3jZ2F6l-Xg1LpLW5abcfLy6B' width =550 /><br>"
      ],
      "metadata": {
        "id": "CelMtBe9TZ5O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 맥락으로 $w_{t-1}, w_{t+1}$이 주어졌을 때 $w_t$가 타깃이 될 확률을 수식으로 쓰면 다음과 같다."
      ],
      "metadata": {
        "id": "3RZ9xnYTTbrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ZZk-4AekpOYQ69x-483iAYVpAX08vbJD' width = 550 /><br>"
      ],
      "metadata": {
        "id": "M_PFxR3PTw7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+  $w_{t-1}, w_{t+1}$가 주어졌을 때  $w_t$가 일어날 확률을 뜻한다.\n",
        "+ 즉 CBOW는 위 식을 모델링 하고 있는 것이다."
      ],
      "metadata": {
        "id": "E-bYgERATzo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 식을 이용하면 CBOW 모델의 손실 함수도 간결하게 표현할 수 있다.\n",
        "+ 교차 엔트로피 식을 적용해보자."
      ],
      "metadata": {
        "id": "Eg_1Ci8ST8um"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ \\huge L= -Σ_t t_klog y_k$"
      ],
      "metadata": {
        "id": "BmmbntSTU70M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 이때 $y_k$는 k번째에 해당하는 사건이 일어날 확률이다.\n",
        "+ $t_k$는 정답 레이블이며 원핫 벡터로 표현된다.\n",
        "+ 여기서 문제의 정답은 $w_t$가 발생하는 것이므로 $w_t$에 해당하는 원소만 1이고 나머지는 0이 된다.\n",
        "+ 즉 다음 식을 유도할 수 있다."
      ],
      "metadata": {
        "id": "GlUkAWRxUQCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1R4nqbAkNihfF1zs7RcVNNi5yqQe35HDa' width = 550/><br>"
      ],
      "metadata": {
        "id": "pEXz3YhVVRPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델의 손실 함수는 단순히 [식 3.1] 확률에 log를 취한 후 마이너스를 붙이면 된다.\n",
        "+ 이를 음의 로그 가능도 라고 한다.\n",
        "+ 이를 말뭉치 전체로 확장하면 다음 식이 된다."
      ],
      "metadata": {
        "id": "XXjz3jdYVWqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1g0BVcWh27CIk_W_iNj2QqIl-T9YBj936' width = 550/><br>"
      ],
      "metadata": {
        "id": "_bC-YjwGVHnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델의 학습이 수행하는 일은 이 손실 함수의 값을 가능한 작게 만드는 것이다.\n",
        "+ 이때 가중치 매개변수가 우리가 얻고자 하는 단어의 분산 표현이다."
      ],
      "metadata": {
        "id": "Ff6N_rhTVkG6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.2 skip-gram 모델"
      ],
      "metadata": {
        "id": "eq_s7O0JVs6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ word2vec은 2개의 모델을 제안하는데 하나는 CBOW 모델이고 다른 하나는 skip-gram 모델이다.\n",
        "+ skip-gram은 CBOW에서 다루는 맥락과 타깃을 역전시킨 모델이다."
      ],
      "metadata": {
        "id": "spTfEjWjVuz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Vr1Ck9NP1x5tLIxoym0ISIC7dXA5GYqX' width = 550/><br>"
      ],
      "metadata": {
        "id": "svwDnaiyVysr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델은 맥락이 여러 개 있고 여러 맥락으로부터 중앙 단어인 타깃을 추측한다.\n",
        "+ skip-gram 모델은 중앙 단어인 타깃으로부터 주변의 여러 단어인 맥락을 추측한다.\n",
        "+ skip-gram의 신경망 구성은 다음과 같다."
      ],
      "metadata": {
        "id": "wg-Yn94PWBV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1PX9fGmmYI_iKg-RdovvmO5gkhidX23Hi' width = 550/><br>"
      ],
      "metadata": {
        "id": "sYWzrp0rWXkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ skip-gram 모델의 입력층은 하나이고 출력층은 맥락의 수만큼 존재한다.\n",
        "+ 따라서 각 출력층에서 개별적으로 손실을 구하고 이 개별 손실들을 모두 더한 값을 최종 손실로 한다."
      ],
      "metadata": {
        "id": "1ny2AYtHWZo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ skip-gram 모델의 확률 표기로 나타내보자.\n",
        "+ 타깃 $w_t$로부터 맥락 $w_{t-1}, w_{t+1}$을 추축하는 경우를 생각해보면 다음 식을 모델링하게 된다."
      ],
      "metadata": {
        "id": "NnWqu1W1Wm29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1FEM-7GS0PehDXFPfwtzN8ohRzebOitxd' width = 550/><br>"
      ],
      "metadata": {
        "id": "DetKrH_UXAx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 식은 $w_t$가 주어졌을 떄 $w_{t-1}, w_{t+1}$이 동시에 일어날 확률을 뜻한다.\n",
        "+ skip-gram 모델에서는 맥락의 단어들 사이에 관련성이 없다고 가정하고 다음과 같이 분해한다."
      ],
      "metadata": {
        "id": "RwVYyh1vXCqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1jFbINLxvO-FOR3QaMUsZf_XfEukzv0aw' width = 550/><br>"
      ],
      "metadata": {
        "id": "3OmrKrdKXdFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 위 식을 교차 엔트로피 오차에 적용하여 skip-gram 모델의 손실 함수를 유도할 수 있다."
      ],
      "metadata": {
        "id": "RetHhv4XXqIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1ayNxRQuedMS6Z5QInxaPXlDo0-mtU29q' width = 550 /><br>"
      ],
      "metadata": {
        "id": "ImLMsW-HXdeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ skip-gram 모델의 손실 함수는 맥락별 손실을 구한 후 모두 더한다.\n",
        "+ 이를 말뭉치 전체로 확장하면 skip-gram 모델의 손실 함수는 다음 식이 된다."
      ],
      "metadata": {
        "id": "Ggy5qcK2Xub9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='http://drive.google.com/uc?export=view&id=1Pp80FMg7ISsebqi8l-LhZ3vDvpJ9IQYr' width = 550 /><br>"
      ],
      "metadata": {
        "id": "JrvuCpm0Xg-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ skip-gram 모델은 맥락의 수만큼 추측하기 대문에 손실 함수는 각 맥락에서 구한 손실의 총합이다.\n",
        "+ CBOW 모델은 타깃 하나의 손실을 구한다."
      ],
      "metadata": {
        "id": "MlwBQqhEX4Tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ CBOW 모델과 skip-gram 모델 중 어느 것이 더 좋은 모델일까?\n",
        "+ 정답은 skip-gram 모델로 단어 분산 표현의 정밀도 면에서 skip-gram 모델의 결과가 더 좋은 경우가 많다.\n",
        "+ 학습 속도 측면에서는 CBOW 모델이 더 빠른데, skip-gram 모델은 손실을 맥락의 수만큼 구해야 해서 계산 비용이 커지기 때문이다.\n",
        "+ skip-gram 구현에 대해서는 따로 설명하지 않으니 ch03/simple_skip_gram.py에 구현을 참고하자."
      ],
      "metadata": {
        "id": "eoSK4xIgYBAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5.3 통계 기반 vs 추론 기반"
      ],
      "metadata": {
        "id": "ZV0D8KQoYUiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 통계 기반 기법은 말뭉치의 전체 통계에서 1회 학습하여 단어의 분산 표현을 얻는다.\n",
        "+ 추론 기반 기법은 말뭉치를 일부분씩 여러 번 학습했다.(미니배치 학습)\n",
        "+ 학습 방법 외의 두 기법의 차이를 살펴보자."
      ],
      "metadata": {
        "id": "353tTcIHY8BL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 어휘에 추가할 새 단어가 생겨 단어의 분산 표현을 갱신해야 하는 상황을 생각하자.\n",
        "+ 통계 기반 기법은 처음부터 다시 계산해야 한다.\n",
        "+ 단어의 분산 표현을 수정하려면 동시발생 행렬을 다시 만들고 SVD를 수행해야 한다.\n",
        "+ 추론 기반 기법은 매개변수를 다시 학습할 수 있다.\n",
        "+ 학습한 가중치를 초깃값으로 사용해 학습하면 돼서 효율적으로 단어의 분산 표현을 갱신할 수 있다."
      ],
      "metadata": {
        "id": "QfmsyOhZZKpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 정밀도 면에서 비교해보자.\n",
        "+ 통계 기반 기법은 단어의 유사성이 인코딩된다.\n",
        "+ word2vec에서는 단어의 유사성과 복잡한 단어 사이의 패턴까지 파악되어 인코딩된다.\n",
        "+ 이런 이유로 추론 기반 기법이 통계 기반 기법보다 정확하다고 생각할 수 있지만  \n",
        "실제 실험에서는 둘 사이의 우열을 가릴 수 없다고 한다."
      ],
      "metadata": {
        "id": "0rjARnZXZipt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 추론 기반 기법과 통계 기반 기법은 서로 관련되어 있다고 한다.\n",
        "+ skip-gram과 네거티브 샘플링을 이용한 모델은 모두 말뭉치 전체의 동시발생 행렬에  \n",
        "특수한 행렬 분해를 적용한 것과 같다\n",
        "+ word2vec 이후 두 기법을 융합한 GloVe 기법이 등장하였다."
      ],
      "metadata": {
        "id": "n1-nhn1iZ17d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 정리"
      ],
      "metadata": {
        "id": "eHlR-r65Yke0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ 추론 기반 기법은 추측하는 것이 목적이며, 그 부산물로 단어의 분산 표현을 얻을 수 있다.\n",
        "+ word2vec은 추론 기반 기법이며 단순한 2층 신경망이다.\n",
        "+ word2vec은 skip-gram 모델과 CBOW 모델을 제공한다.\n",
        "+ CBOW 모델은 여러 단어로부터 하나의 단어를 추축한다.\n",
        "+ skip_gram 모델은 하나의 단어로부터 여러 단어를 추측한다.\n",
        "+ word2vec은 가중치를 다시 학습할 수 있으므로 단어의 분산 표현 갱신이나 새로운 단어 추가를 효율적으로 수행할 수 있다."
      ],
      "metadata": {
        "id": "blLFB38CYl_u"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhC1A7pKYlSg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}